# ğŸ•·ï¸ Scrapy - Web Scraping com Python

## Projeto scraping empregos no site "empregos.maringa.com"
O Projeto utiliza o Scrapy para navegar e extrair dados de vagas de emprego no site empregos.maringa.com. Ele faz scraping de mÃºltiplas pÃ¡ginas e coleta os seguintes dados de cada vaga:

- TÃ­tulo da vaga
- Nome da empresa
- Data de publicaÃ§Ã£o
- Link para candidatura

Os dados extraÃ­dos sÃ£o retornados como dicionÃ¡rios e podem ser salvos, por exemplo, em CSV com:

## ğŸ“Œ O que Ã© o Scrapy?

[Scrapy](https://scrapy.org/) Ã© um framework **open-source em Python** para **extraÃ§Ã£o de dados de websites** de forma rÃ¡pida, eficiente e estruturada. Com ele, vocÃª pode criar spiders (robÃ´s) que navegam e coletam dados automaticamente de pÃ¡ginas da web.

---

## ğŸš€ Vantagens do Scrapy

- âš¡ Alta performance com suporte a **concorrÃªncia**
- â™»ï¸ ReutilizaÃ§Ã£o de cÃ³digo com middlewares e pipelines
- ğŸ“„ ExportaÃ§Ã£o fÃ¡cil para CSV, JSON, XML, banco de dados etc.
- ğŸ”§ Estrutura modular e altamente personalizÃ¡vel
- ğŸ•¸ï¸ Ideal para projetos pequenos ou grandes de scraping

---

## ğŸ§° InstalaÃ§Ã£o

### **Recomendado usar um ambiente virtual:**
Na pasta do projeto, digite os seguintes comandos para criar o ambiente virtual

```bash
python -m venv .venv
source .venv/bin/activate          # Linux/macOS
source .venv\Scripts\activate      # Windows
```
### Instale com pip:
```bash
pip install scrapy
```

## ğŸ—‚ï¸ Estrutura de Projeto
Para iniciar um projeto, Ã© necessÃ¡rio digitar o seguinte comando:
```bash
scrapy startproject scarpy_empregos
```

Segue abaixo a estrutura utilizada no projeto:
```bash
scrapy_empregos/
â”œâ”€â”€ .venv/
â”œâ”€â”€ .git/
â”œâ”€â”€ data/ # Local onde os dados serÃ£o salvos
â”œâ”€â”€ transformacao/
â”‚   â””â”€â”€ main.py # Etapa onde serÃ¡ realizado o tratamento 
                # e filtrado apenas os registros desejados
â”œâ”€â”€ scrapy_empregos/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py         # Define os dados que serÃ£o extraÃ­dos
â”‚   â”œâ”€â”€ middlewares.py   # LÃ³gica intermediÃ¡ria da requisiÃ§Ã£o
â”‚   â”œâ”€â”€ pipelines.py     # Processamento dos dados extraÃ­dos
â”‚   â”œâ”€â”€ settings.py      # ConfiguraÃ§Ãµes do projeto
â”‚   â””â”€â”€ spiders/         # Onde ficam os spiders (robÃ´s)
â”‚       â””â”€â”€ empregos.py.py
â””â”€â”€ scrapy.cfg

```

### Executar o spider
Para executar o spider, Ã© necessÃ¡rio estar dentro do diretÃ³rio ```scrapy_empregos/scrapy_empregos``` e executar o seguinte comando:

```bash
scrapy crawl empregos
```

Executar o spider e salvar os resultados em um arquivo json:
```bash
scrapy crawl empregos -O ../data/data.csv
```